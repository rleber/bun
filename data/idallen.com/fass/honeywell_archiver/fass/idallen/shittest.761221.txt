


.1...1..
fass























.so idallen/roffinit
.sp20   
.bh 
A SHORT HEURISTIC INFORMATION TEST  
.sp2
.ce 
Construction, Administration, and Analysis  
.sp22   
.ta60R  
PSYCHOLOGY	IAN D. ALLEN 
PSYCH 285	74210779 ARTS 
PROF. PSOTKA	N-5, 314, V-1  
.pa1
.ds 
.he''- % -''
.bh 
I: Design Objectives & Test Construction    
.ix 
I:~Design~Objectives~&~Test~Construction~	^%
.en 
.qb2
A NOTE ON TEST CONSTRUCTION: This test was administered as a subset 
of a 78-item test. This test constituted the even-numbered items thereupon. 
Reference will be made to this 78-item test, as well as the 39-item odd half
which was analysed by another student, Cheryl Garnick. The 78-item test was 
constructed from a common pool of items, and administered by each of us.
Item analysis and following discussion, however, were conducted independently.  
.qe2
.bs 
The Criteria:   
.ix 
~~~~~The~Criteria~	^%   
.en 
.pb 
The test booklet asked for three criteria: Grade 13, first year, and
current year academic standing. 
It was expected that the general nature of the test might best correlate
with Grade 13 marks, which would in most cases be dependent on a more   
general course background than the marks received for specialized subjects  
in university. (This expectation was later confirmed.)  
.bs 
Construct Validity: 
.ix 
~~~~~Construct~Validity:~	^%
.en 
.pb 
The dependency of the test on general knowledge might have been reduced had 
we been able to measure 
.ul 
reasoning   
capacity rather than simply knowledge.  
Due to the difficulty in constructing reasoning items, and the longer   
length of time taken to answer such items, we were forced to limit their
inclusion in the test.  
We were forced to assume that current   
.ul 
knowledge,  
especially current verbal   
ability, would be based on past reasoning ability (IQ?) and therefore a quick   
measure of same.
Had we more time to construct and administer the test, this assumption need 
not have been made. 
.bs 
Speed:  
.ix 
~~~~~Speed:~	^% 
.en 
.pb 
The test was not speeded, to simplify the item analysis.
Due to the busy time of year, we sought to make construction, administration,   
and taking the test as brief as possible.   est This led to multiple-choice items and short, fac

This led to multiple-choice items and short, factual questions. 
Long definitions and explanations were thus avoided, and scorer reliability 
was improved due to the objective nature of the answers.
A group test was indicated, since we could not spend the time required  
to give individual attention to each testee.
(Naturally, this meant that individual difficulties were not detected, plus 
incurring the disadvantages of group testing [Anastasi, p. 301].)   
The pencil and paper format of the test permitted us to duplicate the test  
and give it to several persons at one time, and to give it to people to 
complete at their leisure.  
.m3-2   
(This latter laxity relied on the testee's honesty not to cheat. We 
believe that, given the fun nature of the test and our rapport  
with the testees, we were sucessful in convincing people to refrain from
such cheating. The generally low nature of most scores would also tend to   
confirm this. Were time and facilities available, we could have overseen
each testee, and even have administered an individual test.)
.bs 
Content:
.ix 
~~~~~Content:~	^%   
.en 
.m3+2   
.pb 
We were to rely on the item analysis to eliminate useless items,
and consequently strove to amass a large pool from which many deletions 
could be made.  We attempted to cover various fields: english, math, art,   
science, history, chemistry, and general knowledge.  Emphasis was on
verbal items, since these appeared (from the literature) to have the
best chance at predicting academic success, which was our criterion.
Work with a dictionary (Webster's) and a thesaurus (Roget's) provided   
most of these items.  Others came from friends (who were later excluded from
taking the test, for that reason).  
In keeping with Anastasi (p. 416) we provided "clang" associative words 
for most synonym and antonym items, to distract from the actual response.   
.pb 
There were no spatial or performance items on the test, although we did 
initially try to construct some.  The task proved too time-consuming,   
especially in originating unique and different items.  Our secondary
consideration was the difficulty which the computer would have in   
printing these items.  The performance items would have required the presence of
the examiner, which would have eliminated the advantages of the group test format.  
.bs 
Face Validity:  
.ix 
~~~~~Face~Validity:~	^% 
.en 
.pb 
The test certainly  
.ul 
read
like a test; however, it was hard to see
(even for us) how this motley selection of 78 items could possibly correlate
with anything, especially academic achievement or intelligence! 
This was likely due to the test's necessary emphaasis on facts  
rather than thinking process (knowledge rather than reasoning). 
Certain items were designed to be fun (e.g. #3), others to be   
somewhat novel and make one think (e.g. #6, 13).  General information   
items were placed first, to stimulate interest and familiarity. These were  
followed by the difficult verbal section, and finally by the easier 
(too easy, it appears) math section.
.bh 
II: Administration & Scoring
.ix 
II:~Administration~&~Scoring~	^%
.en 
.pb 
When possible, the testee was overseen while taking the test. The total 
time it took subjects to complete all 78 items ranged from 20 to 45 minutes.
Rapport with testees was good; many found the test "hard" but   
enjoyed the challenge.  
The nature and purpose of the test was stated on the cover of the test booklet. 
.pb 
Scoring was on a strict pass/fail basis (no part marks given)   
with one mark per item. 
The multiple choice items could have been scored on a computer  
had the facilities been available for optical card reading. As it was   
the test was hand scored, and the results tallied into a computer file  
for later processing.  Codes were used to input the Faculty,
sex, and mother tongue for each testee (see key).   
.pb 
The computer program was written from scratch for this projects (with   
the exception of the scatter plot and histogram subroutines) by 
Ian Allen.  Output for each run gives normative sample statistics   
(mother tongue, age, sex, etc.), and item analyses (based on p. 209-210 
of Anastasi) using 1) Grade 13 criterion (field 6), 2) 1st Year criterion   
(field 7), 3) current year criterion (field 8), and 4) test score   
criterion (field 86 or 90 depending on run).
Scatter plots were produced for 1, 2, and 3 (analysis 4 is simply   
a straight line, test vs test). 
The fourth analysis gives an indication of internal consistency, which  
turned out to be higher than the validity against any of the criteria.  
The test consistently tested something - but it wasn't any of the criteria! 
.pb 
Splitting the original 78-item test into the 39-item even and   The 39-item odd halv

39-item odd halves increased the variance from 148.55 (78-item) to  
199.87 (39-item even) (see histograms). 
.bh 
III: Item Analysis & Validation 
.ix 
III:~Item~Analysis~&~Validation~	^% 
.en 
.pb 
Statistics for the undivided 78-item test are provided for interest.
Correlations between it and the resulting "daughter" tests is as follows:   
.qb10   
.nf 
odd vs. even       : .712   
odd vs. full 78    : .911   
even vs. full 78   : .937   
.fi 
.qe10   
.pb 
Intercorrelations between the three chosen criteria were rather low:
.qb10   
.nf 
Grade 13 vs. current year  : .535   
1st year vs. current year  : .693   
.fi 
.qe10   
.pb 
The mean D of the test remained relatively constant for all three   
criteria after the split, but the correlations of the test score against
1st year and current year dropped (see analyses).   
The 39-item test seemed to best correlate with Grade 13 marks, and I decided
that this would be the criterion to use in throwing out items.  
.bs 
First deletions - 39 to 28 items:   
.ix 
~~~~~First~deletions~-~39~to~28~items:~	^%  
.en 
.pb 
Items were deleted from the 39-item test using the following two rules: 
.qb10   
.nf 
1) item has a negative D (#4, 36, 44, 56, 64, 68, 70)   
2) item has a  zero    D (#8, 30, 40, 52)   
.fi 
.qe10   
There appeared no consistent pattern to these items.
Statistics were run on the resulting 28-item test:  
.qb10   
.nf 
.ta22L+10L  
new mean D	is +8	= 20   
new r with Grade 13	is +.154	= .517 
new variance	is +25.38	= 225.25 
.fi 
.qe10   
.pb 
The increased mean D indicates an increased test reliability (Anastasi, p.  
212). The increased mean D in the 4th analysis (internal consistency)   
would indicate greater internal consistency as well.
.bs 
Second Deletions - 28 to 25 items:  
.ix 
~~~~~Second~Deletions~-~28~to~25~items:~	^% 
.en 
.pb 
I first removed 3 items with a low D in both the first (Grade 13)   
analysis and the fourth (internal consistency) analysis: items  
#32, 58, 76.  This increased the mean D, as predicted, but  
.ul 
decreased   
the correlation against the Grade 13 criterion to .455! 
I checked the items again, replaced item #76, and deleted #18 instead.  
Item 18 was "backwards" in its discrimination between the Middle and Lower  
criterion groupings, more so than item 76. The new analysis went as follows:
.qb10   
.nf 
new mean D	is +1	= 21   
new r with Grade 13	is -.003	= .514 
new variance 	is +32.75	= 258.0 
.fi 
.qe10   
.pb 
Deleting an item which does discriminate, even if only by   
10%, may lower the correalation against the criterion by removing   
another item which is weighted in the correct direction.
.pb 
Again, internal consistency rose.  Correlations of the 25-item test 
with its "parent" tests are as follows: 
.qb10   
.nf 
25-item with odd-39  : .722 
        with even-39 : .90  
        with full-78 : .88  
.fi 
.qe10   
.pb 
The first measure may be interpreted as a form of "alternate-form   
reliability", except that the odd-half has more items and has not   
been refined as this 25-item test has.  The latter two correlations 
indicate the dissimilarity between this refined test and each of its
successive "parents".   
.bh 
IV: Discussion  
.ix 
IV:~Discussion~	^%  
.en 
.pb 
Our "random" sample of the university poplation ended up heavily
biased toward 1) males and 2) science/math/engineering students.
The math items on the test failed to discriminate among such a capable  
population, nor even among arts students. Since ___all did well  
on the math (high p-values), it did not really compensate for those 
who were verbally disadvantaged but mathematically capable. 
.pb 
The small number of females present in the sample (7) makes it  
difficult to draw sex difference conclusions.  The small number of  
persons with a mother tongue other than english prevents a worthwhile   
ethnic bias analysis.   
In either case, each category is confounded with every other, and meaningful
results can only be done by partialling out variances and holding factors   
constant; this would be elicited from a factor analysis or multiple 
ANOVA.  
.pb 
The test appeared to be adequately difficult; a look at the histograms  
shows the score distributions as centering around 57%. Elimination  
of items increased the score distribution (variance), which is good.
.bp 
.bh 
V: OUTLINE EVALUATION OF THE S.H.I.T.   
.ix 
V:~Outline~Evaluation~Of~The~S.H.I.T.~	^%   
.en 
.ss 
.sp2
.nf 
.in5
.ti-5   
A.~~General Information 
.sp 
Authors: Ian D. Allen & Cheryl Garnick  
Time:    30-45 minutes  
.sp 
.ti-5   
B.~~Description 
.sp 
Type:       paper & pencil group intelligence test  
Population: university undergraduates   
Content:    Verbal, some Numerical, some General Knowledge  
Subtests:   none distinguished  
Item Types: multiple choice and short-answer (5 words)  
.sp 
.ti-5   
C.~~Practical Evaluation
.sp 
Examiner Qualifications: none special - high school education?  
Scoring: computer possible for multiple choice  
         examiner necessary for short answer
.sp 
.ti-5   
D.~~Technical Evaluation
.sp 
Norms: may be derived from histogram of scores on test  
Standardization Sample: 7 females, 25 males 
                        Waterloo university undergraduates  
                        5 non-english mother tongue 
                        average school year of 2.7  
                        average age of 20.8 
Selection proceedure:   generally friends and friends   
                        of friends of test authors! 
Reliabilty: using parallel form on same normative sample
            correlation of approx.  .71 (product moment)
Validity:   using same normative sample 
            test correlation with Grade 13:     .514
                             with 1st year:     .229
                             with current year: .332
.sp 
                                -as per Anastasi, Appendix C
